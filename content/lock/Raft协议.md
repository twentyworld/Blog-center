对存储型系统来说，为了消除单点故障、提高可用性，系统会用集群方案部署，而数据则则以副本形式存储在多个节点，那多个节点间的数据如何保证一致？答案是共识算法，它即可以保证分布式系统中的一致性，又能保证在小部分节点（半数减一）发生故障时，系统依旧能够正常对外服务。

可由于`Paxos`算法实在难以理解，`Raft`横空出世！**`Raft`算法的立意就是为了推出一个更容易理解的共识算法**，这一点从它的论文名字就能看出：[《In Search of an Understandable Consensus Algorithm》](https://link.juejin.cn?target=https%3A%2F%2Fraft.github.io%2Fraft.pdf)。

> 实际上，`Raft`也可以看作是`Multi-Paxos`的一种派生实现（`Multi-Paxos`本身只有思想，论文里没有给出实现细节），它沿用了`Multi-Paxos`单`Proposer`的思想，保证了运行期间内只会存在一个`Leader`节点，而且是强`Leader`模式，**宁愿让系统没有`Leader`陷入瘫痪，也不允许出现脑裂现象，对外服务的任意时刻都只能存在一个`Leader`**。

`Raft`和`Paxos`的设计理念相同，都是为了解决分布式系统的一致性问题，可`Raft`为了更易于理解，主要做了两方面的工作：**问题分解、状态简化**，下面展开聊聊。

### 1.1、问题分解

刚刚提到过`Raft`的显著特点：**强`Leader`特性**，使用`Raft`算法的集群，必须存在一个主节点才能工作，因为客户端的所有操作，都会先交给`Leader`节点处理。既然如此，我们能保证运行期间主节点一直不挂掉吗？显然不能，在网络故障、宕机等问题随时都有可能发生的分布式系统里，`Leader`节点也不例外。

既然`Leader`有发生故障的风险，而`Raft`集群又重度依赖于`Leader`节点完成工作，那么`Raft`算法的第一个核心问题就是：**领导者选举（Leader Election）**，只有时刻保证`Leader`节点的可靠性，才能让`Raft`集群正常处理外部请求。

通过选主机制保证了`Leader`的稳定性后，因为客户端的操作都会交由主节点处理，所以要实现集群副本之间的一致性，只需要将主节点上的数据操作，同步给集群内其他节点即可，而这就是`Raft`算法第二个核心问题：**日志复制（Log Replication）**。

依靠同步机制保证了副本间的一致性后，由于`Leader`节点责任重大，在选举时必须要慎之又慎！比如一个新节点刚加入集群，此时`Leader`节点刚好宕机，于是集群触发选举机制，巧合之下，刚加入的新节点成为了新`Leader`，这是否合理？`No`，毕竟刚加入集群的新节点，连客户端原先写入的数据都不具备……。正因如此，`Raft`在做领导者选举、日志复制时，必须要有一定的限制，这对应着`Raft`算法第三个核心问题：**安全性（Safety）**。

综上所述，`Raft`对共识算法进行了拆解，从中分解出领导者选举、日志复制、安全性这三个子问题，围绕这三个子问题展开，构成了`Raft`算法的核心内容，而这三部分的具体内容，我们放到后面一起讨论。

### 1.2、状态简化

`Raft`除开将共识算法分解成三个子问题外，还简化了`Paxos`中的状态，在`Paxos`算法中定义了`Proposer、Acceptor、Learner`三种角色，并且这些角色并不会固化在某个节点上，`Paxos`集群中的任意节点，即可能是`Proposer`提案者，也有可能是`Acceptor`接受者或`Learner`学习者。

因为节点的角色会动态变化，所以集群内的状态流转会额外复杂，相同时间的任意节点，都能发起提案、批准提案、学习提案，这也说明`Paxos`是一种`P2P`算法，集群每个节点的地位、能力都是平等的！`Paxos`这种思想十分先进，可实现起来额外困难，而`Raft`算法中则简化了集群内的状态。

`Raft`算法通过选举出`Leader`节点，**从而将`Paxos`的`Proposer`角色固化在一个节点上**，在运行期间内，只允许一个节点发出提案，剩余节点只能被动接受、学习提案，来看这张摘自`Raft`论文里的原图：

![角色切换](Raft协议.assets/0c4f4ce18a564000826ec5c9b8fbf825~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

在同一时刻，`Raft`集群的各节点会固化一种角色，即一个节点只会属于`Leader、Follower、Candidate`其中一种角色！相较于`Paxos`，这种模式就极大程度上减少了算法的状态数量，及可能产生的状态变动，**毕竟`Raft`只需要考虑选举期间发生的角色切换，而不需要像`Paxos`那样考虑多种角色的共存和互相造成的影响**。

关于`Raft`协议的三个核心内容，以及几种角色之间的切换，聊到这里暂且打住，`Raft`本质是基于“状态机”工作的一种算法，而究竟什么叫状态机呢？一起来看看。

### 1.3、复制状态机

`Replicated State Machine`复制状态机是一种抽象的概念，主要目的是用于确保分布式系统中各节点其副本状态的一致性。`Raft`复制状态机由多个复制单元组成，每个复制单元存储一个包含一系列指令的日志，来看论文中给出的示意图：

![状态机](Raft协议.assets/de983eff4fd640fc833ed60c2c9dee67~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

上面层叠的黄色卡片，就是一个个复制单元，说人话就是`Raft`集群里的一个个节点。当客户端向`Raft`服务集群发起请求时，`Raft`会将对应的操作封装成日志（`Log`），如果存在多个操作，则会将不同的操作封装成一个个`Entry`节点放入到日志里。正如上图所示，`Log`由多个`Entry`组成，`x←3`，表示将`x`这个变量赋值为`3`，而客户端一系列操作对应的`Log`，最终会被放到`Leader`节点的`State Machine`状态机中。

状态机其实是个用于描述程序行为的数学模型，属于学术界的名词，举例说明，假设目前`x=1`，当客户端向服务端发起一个`x←3`的操作后，服务端存储的`x`值，就会从初始态（`1`）变成结束态（`3`），这就是一种状态机的表现，而`Raft`中的复制状态机，即是指将`Leader`节点的状态机，应用到所有复制单元（每个节点）上，在分布式系统中：

> **如果一开始所有节点的副本数据都一致（状态相同），执行同样的操作指令后，最终的副本状态肯定也一致**。

当然，这里说的“同样的操作指令”，指的是执行顺序也要一致！如上图中的案例，在`Leader`节点依次执行`x←3、y←1、y←9`指令后，剩余节点也依次执行这三个指令，那么最终所有节点的数据必然是`x=3、y=9`，因此，**只要实现了复制状态机，就能够保证分布式系统的数据一致性，客户端不管从哪个节点读取，都能看到相同的结果**。

### 1.4、Raft角色与转变

简单理解复制状态机的概念后，接着来看看`Raft`定义的三种节点角色/状态：

![三种角色](Raft协议.assets/0c4f4ce18a564000826ec5c9b8fbf825~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

大家暂时先别看图中的箭头，先看看图中的三种角色：

- `Leader`领导者：负责处理客户端的所有操作，并将操作封装成日志同步给集群其他节点；
- `Follower`追随者：负责接收`Leader`节点封装好的日志，并应用于自身的状态机里；
- `Candidate`候选者：当集群中没有`Leader`节点时，追随者会转变成候选者，尝试成为新`Leader`。

如果对分布式技术有所掌握的小伙伴，对这三个概念并不陌生，这是所有主流技术栈中都存在的概念，只不过某些地方叫法不同罢了。不过值得说明的一点是，**`Follower`追随者本身并不具备处理任何客户端请求的能力，当`Follower`接收到外部请求时，会将客户端请求重定向到`Leader`处理**，只是在有些技术栈里，为了充分利用空闲资源，会允许`Follower`处理读类型的操作，毕竟读操作并不会引起状态变化。

简单了解三种节点角色后，各位再把目光聚焦到图中的箭头上，这是指集群节点的角色转变过程，在集群启动时，所有节点都为`Follower`类型，而根据起初的定论，`Raft`集群必须要有一个`Leader`节点来处理外部的所有操作，为此，**在集群启动后就会出现第一轮选主过程**。`Raft`中为了区分每一轮选举，定义了一个概念：**`term`（任期）**。

每一轮新的选举，都被称为一个任期，每个`term`都有一个唯一标识来区分，这个标识就叫做任期编号，并且与`Paxos`的提案编号具备相同属性，**必须要保证严格的递增性**！即第二轮选举对应的任期编号，一定会大于第一轮选举的任期编号。

集群启动会触发第一轮选举，对应的任期编号为`1`，选举的过程也不难理解，当一个`Follower`节点发现没有`Leader`存在时，自己会转变为`Candidate`节点，先投自己一票，接着向其他节点发起拉票请求，如果得到了大多数节点（半数以上）的节点支持，此`Follower`节点就会成为本轮任期中的`Leader`节点。

上面这个过程，就完成了`Follower、Candidate、Leader`三种角色的转变，听起来似乎很简单，可是却存在很多细节，如：

- `Follower`是如何感知到没有`Leader`存在的？
- `Candidate`是怎么向其他节点拉票的？
- 如果多个`Follower`一起转变成`Candidate`拉票怎么办？
- 新`Leader`上线，其他节点是如何成为其追随者的？
- ……

这些细节就构成了`Raft`选举的核心知识，下面来展开聊下`Raft`算法的领导者选举机制。

## 二、Raft领导者选举（Leader Election）

开始一轮新选举的前提是要感知到没有`Leader`存在，具体是如何实现的呢？很简单，就是之前提过无数次的心跳机制，心跳机制建立在通信的基础之上，所以`Raft`也是一种基于消息传递的共识算法。

在`Raft`协议中，心跳包、日志复制、拉票/投票等消息的传递，都依赖`RPC`来做通信，主要分为两大类：

- `RequestVote RPC`：用于选举阶段的拉票/投票通信；
- `AppendEntries RPC`：用于存在`Leader`时期的日志复制、发送心跳。

`Raft`依托这两类`RPC`完成集群工作，不管是哪类`RPC`，在通信时都会携带自己的任期编号，通过附带的任期号，就能将所有节点收敛到一致状态，如`Leader、Candidate`收到一个`RPC`时，发现大于自己的编号，说明自身处于过期的任期中，此时就会自动转变到`Follower`角色。

不过这些细节先不展开，我们先来看下前面给出的第一个疑惑：`Follower`是如何感知到没有`Leader`的？答案是心跳机制。

### 2.1、Raft心跳机制

`Raft`中的心跳包只能由`Leader`发出，作用主要有两点，**其一是帮助`Leader`节点维持领导者地位**，持续向集群内宣布自己还“活着”，防止其他节点再次发起新一轮的选举；**其二是帮助`Leader`确认其他节点的状态**，如果某个节点收到心跳包后未作回复，`Leader`就会认为该节点已下线或陷入故障了。

![Raft心跳](Raft协议.assets/6a08ccd6c3a24e81bd3b998e883a2c6b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

结合上述知识来看前面的问题，集群只要有`Leader`存在，就一定会有心跳包发出，刚启动时，因为所有节点的角色都是`Follower`，这意味着不会有节点收到心跳包，因此，`Follower`会认为领导者已经离线（不存在）或故障，接着就会发起一轮新的选举过程。

> PS：一轮选举/一个`term`中，一个节点只能投出一票！

再来看个问题，最开始所有节点都是`Follower`，如果同时检测到`Leader`不存在，一起转变成`Candidate`开始拉票怎么办？

![选举僵持](Raft协议.assets/61bb1c383c664e29a1e512096aac864d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

上图中，五个节点都持有自己的一票，没有任何节点胜出，本轮选举就会陷入僵局，而`Raft`自然考虑到了这种局面，所以对`term`任期附加了一个条件：**一轮任期在规定时间内，还未票选出`Leader`节点，就会开始下一轮`term`**！不过仅这一个条件还不够，毕竟新的一轮选举中，各节点再次一起拉票，又会回到互相僵持的局面……

如果一直处于这种情况，会导致`Raft`选主的过程额外低效，因此，为了跳出这个死循环，`Raft`给每个节点加了随机计时器，**当一个节点的计时器走完时，依旧未收到心跳包，就会转变成`Candidate`开始拉票**，因为这个计时器的值存在随机性，所以不可能全部节点一起转变成`Candidate`节点，这就从根源上解决了前面的僵持性问题。

> PS：这个随机计时器，也会成为一轮选举的超时限制，`Raft`论文给出的建议是`150~300ms`，后续再做展开。

不过就算加上随机计时器后，也不一定能`100%`保证同时不出现多个`Candidate`，**`Raft`接受多个`Candidate`同时拉票，但只允许一轮选举中只有一个节点胜出**！咋做到的？依靠集群大多数节点的共识，只有拿到半数以上节点的投票，对应的`Candidate`才有资格成为本轮任期中的`Leader`。

`Candidate`过多会导致票数过于分散，比如由九个节点组成的集群，同时出现四个`Candidate`拉票，各自的票数为`2、3、1、3`，没有任何节点的票数满足“半数以上”这个条件，因此本轮`term`不会有`Leader`产生：

![term](Raft协议.assets/d8655bbc6236411887de5df991511e66~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

`Raft`的一轮任期只会存在一个`Leader`，当出现票数过于分散的场景时，允许一轮没有`Leader`的`term`存在，经过一定时间的推移后，整个集群会自动进入下一轮选举。当然，关于如何推进到下一轮选举，下面详细聊聊。

### 2.2、Raft选举过程

我们先从集群启动开始，对`Raft`几种领导者选举过程进行展开，为了便于理解，这里用到了一个[Raft动画网站](https://link.juejin.cn?target=https%3A%2F%2Fraft.github.io%2F)，大家感兴趣可以自行点开探索。

#### 2.2.1、集群启动选举过程

先看下图：

![集群启动](Raft协议.assets/e5ff39d456d34d4b897dbe698900b1ea~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

上图是由`S1~S5`五个节点组成的集群，大家会发现每个圆圈中间有个数字`1`，它代表着当前的`term`编号；其次，还可以发现，每个圆圈周围都有不规则的“圆形进度条”，这则对应着前面所说的随机计时器！

因为目前刚刚启动，不存在`Leader`节点，所以集群内不会有任何节点发送心跳包。**当集群节点的进度条走完时，如果还未收到心跳，它就会认为`Leader`离线，而后转变成`Candidate`节点，投自己一票并开始拉票**。`Raft`为不同节点的计时器，其倒计时数值加入了一定的随机性，从而避免多个节点一起拉票造成票数分散，集群迟迟无法选出`Leader`的情况。

上图中，`S2`节点的倒计时最短，它会成为第一个发现`Leader`离线、并发起拉票的节点，如下：

![S2拉票](Raft协议.assets/abd3a8981a6541f884700e0c0509f7e1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

`S2`节点发现`Leader`离线后会开始拉票，来看图中的几种变化：**首先`S2`节点转变了颜色**，意味着角色从`Follower`转变成了`Candidate`；**其次，`S2`中间的数值从`1`变为`2`**，代表`S2`推动了一轮新选举，集群进入第二轮`term`；**同时，`S2`内部出现了五个小圈**，这代表集群各节点的投票状态，其中有一个全黑的小圈则是自己的一票；**最后，`S2`发出四个绿色箭头**，这就是发往其他节点的拉票请求。

> 具体的过程算法过程，就是`S2`发现领导者离线，先从追随者转变为候选人，接着自增自身的任期编号，然后投自己一票，最后向集群其他节点发出`RequestVote-RPC`，在该`RPC`对应的数据包里，会携带`S2`自增后的任期编号（`2`）。

![S3投票](Raft协议.assets/0e019476212f40779cbfc113117b8bb9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

随着拉票的`RPC`到达`S3`节点，`S3`节点会先对比`RPC`里携带的任期编号，如果比自身的编号要大，说明发出`RPC`的`S2`节点，**其任期要比自己新，`S3`就会将自己的票投给`S2`，同时将自身的任期编号更新成`2`，并重置自己的计时器（再次获取一个固定范围内的随机超时时间）**。

![S2胜出](Raft协议.assets/7bd648a4c1004ce8a6e4d5fb8c411feb~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

尽管`S5`的投票还未抵达`S2`，但当`S2`收到半数节点以上的投票后，就说明它获得了大多数节点的“拥戴”。观察上图，`S2`的名字变为红色，说明它从`Candidate`转变成了`Leader`节点；同时，它又发出了四个橙色箭头，这就是成为`Leader`后发出的心跳包，**`S2`会依靠心跳维持自己的领导地位，避免其余节点再次开启新一轮选举**。

好了，上面便是集群启动后，第一轮选举的正常流程，接着再探讨两个细节问题：

- `S2`的拉票请求未到达某个节点（如`S4`）时，`S4`也感知到`Leader`掉线发起选举怎么办？
- `S2`发出拉票请求后立马宕机，或`S2`收到部分节点的票数后宕机怎么办？

##### 细节一：多个节点同时发起选举怎么办？

先看第一个问题，各节点的计数器都存在随机性，但也无法避免两个相邻的随机时间产生，如`S1=51ms、S2=50ms`。在这种情况下，`S2`会率先感知到`Leader`离线，在其一毫秒后，`S1`也会感知到`Leader`离线，两者感知到`Leader`离线的时间很接近。因此，在`S2`拉票请求抵达`S1`之前，`S1`也有可能变为`Candidate`开始拉票，这时谁会成为`Leader`呢？**答案是不确定，需要看谁的`RPC`先抵达其余节点**。

其他节点在投票时，因为会先对比自身的任期号，如果`RPC`携带的编号比自己大，就会更新自身的任期号，并把票投给对应的节点。比如`S2`的请求先到`S5`，`S5`对比发现自己小，就会把任期号从`1`变成`2`，然后给`S2`投票。随后，`S1`的拉票请求也来到`S5`，这时对比`S1`携带的编号（`2`），发现自身编号不比它小，为此就会忽略`S1`的拉票请求。

结合“半数节点以上的票选”这个条件，目前集群总共五个节点，`S1、S2`双方各自持有自身的票数，而剩下的`S3、S4、S5`，要么投给`S1`、要么投给`S3`。由于只剩下了三票，只有`S1、S2`两个候选人，笛卡尔积的结果也只能是`0:3、3:0、1:2、2:1`，不管是哪种结果，加上`S1、S2`自身持有的一票，最终都能满足“半数以上”这个条件，反正总会有一个节点胜出。

> PS：现在大家应该明白为何集群节点数都推荐奇数了吧？因为节点数量为奇数的集群，在同一轮选举出现两个候选人的情况下，可以避免相互僵持的局面发生，保证一轮就能选出`Leader`节点。当然，如果出现两个以上的候选人，还是有可能因为票数过于分散，从而没有任何节点胜出，迫不得已开启新一轮选举。

##### 细节二：节点发起选举后宕机怎么办？

好了，再来看第二个问题，`S2`开始拉票后、未正式成为`Leader`前宕机咋办？其实没关系，大家还记得节点投票的动作嘛？先更新任期编号，再给对方投票，**然后再次获取一个固定范围内的随机超时时间，重置自己的计时器**！

因为投票后会刷新自己的计时器，如果节点投票后，新的一轮倒计时结束，还未收到来自`Leader`的心跳，意味着前面拉票的节点已经掉线，第一个感知到此现象的节点，又会率先开启新的一轮选举。将此说法套入前面的案例中，假设`S2`发出拉票`RPC`后就宕机，`S5`率先感知到`S2`掉线，于是它发起新一轮选举，请问对应的`term`编号是几？

答案是`3`，因为当`S5`投票给`S2`后，就会将自身的任期编号变成拉票`RPC`中的`2`，如果发起新的一轮选举，任期号是在自身编号的基础上`+1`，所以`S5`这轮选举对应的任期则为`3`。

至此，集群启动后的第一轮选举过程，包括`Raft`的大致选举流程就阐述完毕了，下面看看后续`Leader`掉线后的选举过程。

#### 2.2.2、Leader掉线选举过程

当集群启动选出`Leader`后，正常情况下，该`Leader`节点会持续发送心跳维持自己的领导者地位，可天有不测风云，谁也无法保障`Leader`节点一直健康，比如运行期间`Leader`所在的机器发生故障，又或者所在的分区网络出现问题，这时就需要选出新的`Leader`继续带领集群！

还是原来的问题，运行期间`Follower`节点如何感知到`Leader`掉线？其实这个问题和上阶段的细节二类似，`Follower`节点除开在投票后会重置自己的计时器之外，**在收到领导者心跳`RPC`后亦是如此**！

![心跳刷新](Raft协议.assets/f346acfd6b764c05bb603f4abf37c985~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

如上图所示，左图是`Leader`节点`S2`向其余节点发出心跳，右图是`Follower`节点回应`S2`的心跳包，注意观察`S1、S3、S4、S5`周边的进度条变化，大家可明显观察出被重置了！基于此特性，当`Follower`节点回复一次心跳后，计数器走完还未收到下一次心跳，这时对应的节点就会认为`Leader`掉线，从而发起新一轮的选举。

##### Raft动画网站概述

![Raft动画网站](Raft协议.assets/dbcc283a687d4f5cb643034582da5389~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

为了模拟运行期间`Leader`宕机的场景，可以借助前面所说的[Raft动画网站](https://link.juejin.cn?target=https%3A%2F%2Fraft.github.io%2F)，这里可以快速介绍一下，图中总共四块区域，左上的几个圆圈区域，代表组成集群的节点；右边的方块区域，代表选举和日志复制的状态（后续会用到）。下面两根进度条，第一根是时间回溯，可以拖拽来回放集群的“历史”；第二根是时速控制，可以拖拽来控制时间的倍速（快或慢）。

同时，在集群节点上右键，还会出现一个选项列表，对应着有五个功能：

- `stop`：让选中的节点立马宕机，用于模拟节点发生故障；
- `resume`：重置选中节点的计时器（重新获取指定范围内的超时时间）；
- `restart`：重启选中的节点；
- `time out`：让选中的节点其计时器立马跑完。用于模拟超时；
- `request`：模拟客户端向节点发出操作请求（适用于`Leader`节点）。

好了，大概对此网站的功能有基本认知后，下面一起来用它模拟运行期间的各种问题。

##### 运行期间Leader掉线

![S2宕机](Raft协议.assets/e1e1863fed6d4b8fbf5d1a04fdead34a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

左图是通过`stop`手动模拟`S2`宕机，宕机后`S2`不再会发出心跳，于是，随着时间推移，`S5`节点率先超时，此时就会将`term`编号自增到`3`，发起新一轮的选举过程：

![S5胜出](Raft协议.assets/efce43711846462493908b8748e641dc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

`S5`发出的拉票`RPC`，由于携带着最大的任期编号，不出意外，`S5`成为了新`Leader`接替了之前`S2`的工作。看完之前的内容后，这个过程大家应该都能理解，这里不做过多展开，下面看看其他情况。

##### 旧Leader节点复活

`S2`宕机、`S5`经过一轮选举后，成为集群内当之无愧的新`Leader`，可是`S2`掉线时，是带着`Leader`身份宕机的，如果旧主`S2`节点此时“复活”，它会不会与`S5`展开一场“夺嫡大戏”呢？我们来重启`S2`（`restart`）模拟一下：

![S2复活](Raft协议.assets/92e61b0abc7d45a7bf49e657f3ba1ef8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

观察上图，答案很显然，旧主`S2`成为了新主`S5`的追随者！`Why`？还记得之前说到的`RPC`嘛？**每个节点在发出`RPC`时，都会携带自身的任期编号**。上面左图中，`S5`发出心跳会带上自己的编号`3`，当旧主`S2`收到心跳后，一对比自身的编号，发现自己是`2`，就会认识到自己处于“过期的`Term`”，于是，就会将自身编号改为对方的编号，并改变身份成为对方的追随者。

> PS：其实运行过程中还会有许多其他状况，比如`Leader`如果在发出心跳后，由于某个节点网络较差，导致接收心跳包出现延迟，从而导致自己的计时器走完，然后发起一轮新选举怎么办？又好比由五个节点组成的集群，在宕机三个的情况下，还能否正常工作及选出新`Leader`？这些问题靠前面的知识很难讲明白，因此暂时不聊，放到后面讲述。

### 2.3、Raft选举小结

经过前面两个阶段，我们分析了多种选举的状态，可不管怎么说，一轮选举的核心就两点：**一是发现`Leader`离线，二是票选出新`Leader`**。当一个节点开始拉票后，能够出现的结果就三种：

- 胜出：收到大多数节点的投票，成为新`Leader`；
- 平局：多个节点一起拉票，导致票数分散，开启下一轮选举；
- 失败：在自己之前有其他节点成为了新`Leader`，收到对方`RPC`后转变成追随者。

理解这三种情况，其实就不难理解`Raft`算法的领导者选举过程了，接着来看看日志复制。

## 三、Raft日志复制（Log Replication）

`Raft`是一种强`Leader`型的一致性算法，上阶段聊到的领导者选举，就是为了让分布式集群选出一个节点来担任集群的领导者。当`Leader`节点上任后，除开要定期向集群发出心跳外，更重要是承接并处理客户端的操作，然后将其同步给集群其他`Follower`节点。

![状态机](Raft协议.assets/de983eff4fd640fc833ed60c2c9dee67~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

最初曾提到，`Raft`算法向`Follower`节点同步数据依靠状态机实现，集群启动时，所有节点的数据（状态机）都处于一致，随着集群持续运行，所有节点经过同样的操作后，最终的数据（状态机）也一定能够达成一致，而这个过程就是所谓的同步状态机。

为了实现同步状态机，`Leader`会把所有客户端的操作（一般泛指写操作），封装成`Log`并加入自己的日志序列，然后再将`Log`同步给所有`Follower`节点。**因为`Log`决定着集群的一致性，所以`Raft`只允许日志从`Leader`流向`Follower`节点**，从而避免`Paxos`那种并发提案冲突造成的不一致现象。

> PS：`Raft`中`Leader`具备绝对的统治力，这种模式被成为`Strong Leader`。

日志只能从`Leader`流向`Follower`，这会引发一个新的问题：`Raft`中的领导者随时可能变更，客户端如何知道`Leader`是谁？方案有好几种：

- ①轮询方案：客户端每次执行写操作时，遍历所有节点，能操作成功就是`Leader`；
- ②重定向方案：集群实现重定向方案，当操作发到`Follower`时，就重定向到最新的`Leader`；
- ③健康检查方案：客户端定期向集群发起一次探测，在客户端维护最新的`Leader`地址。

这三种方案究竟用哪种？这取决于不同的技术栈，`Raft`作为分布式系统的基石，并未对此进行限制。好了，下面进入正题，来聊聊日志复制。

### 3.1、日志复制过程

`Leader`负责承接所有客户端的操作，而后会封装成日志同步给其余节点，先来看张流程图：

![日志同步](Raft协议.assets/832d5fd7b40644daa65e88720de11784~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

如上图所示，当一个写操作抵达`S2`（`Leader`）后，`S2`会将该操作封装成`Log-Entry`（日志条目），并加入到自身的日志序列，然后会通过`AppendEntries-RPC`发往集群其余节点，其他节点收到`RPC`后，也会将`Log`加入到各自的日志序列，同时给`Leader`返回响应。

当`Leader`收到集群大多数节点的响应后，意味着客户端本次操作对应的`Log`已同步至大部分节点，于是，`Leader`会将该日志应用（`Apply`）到自身的状态机，即把数据写入到当前节点，最后身为领导者的`S2`就会向客户端返回“操作成功”。

上述即是完整的日志复制流程，很简单对吧？随着时间推移，整个集群所有节点的日志序列，就会变为成这样：

![日志序列](Raft协议.assets/56178b8a0e0f48a7bc7dddb4ceaea819~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

先来简单解释下图中的每个`Log`，前面说到，客户端的操作会被封装成`Log-Entry`，**但`Log`除开包含客户端的操作外，还会包含当前的任期编号，以及一个在日志序列里唯一的索引下标（日志号）**。图中方块里的数字，就代表是对应`Log`所处的任期（图中颜色相同的日志任期也相同），同理，既然多个日志的任期号相同，说明这些日志都由同一个`Leader`产生。

> PS：通过任期号+日志下标，可以表示图中任何一个日志，如`term3、index6`，即代表`a←1`这个操作。

将目光集中到最下面的`committed Entries`（已提交日志），这是什么含义？诸位回想下前面的日志复制流程：**当`Log`同步给大多数`Follower`节点后，`Leader`会将对应的日志应用于自身状态机，接着向客户端返回操作成功**，而这些成功返回的客户端操作，对应的日志则视为“已提交日志”，还记得[事务ACID原则](https://juejin.cn/post/7152765784299667487#heading-5)里的持久性吗？

> 事务一旦被提交，它将永远生效，即使系统发生故障、出现宕机，也不会影响已提交的事务。

同理，`Raft`中已提交过的日志也具备持久性，`Raft`可以保证提交过的日志永不丢失（即提交的日志一定会被所有节点`Apply`），不过如何实现的，会在后面的安全性章节讲述，这里看下，为何图中最后`term5、index12`这条日志未提交？原因很简单，因为它还未复制到大多数节点，所以无法提交（所谓的提交，即是指`Leader`将日志应用到状态机）。

综上，在`Raft`协议中，如果客户端想要成功完成一个操作，**则至少需要等到大多数节点成功同步日志、且领导者节点成功将日志`Apply`**。不过值得注意的是，日志同步给大多数`Follower`节点后，`Follower`并没有立刻将日志应用于状态机，为啥？因为`Follower`无法确定当前的`Log`，是否已经同步给大多数节点。

集群中唯一能确认`Log`是否可提交的节点是`Leader`，因此，`Follower`节点必须接收到`Leader`的通知后，才能将前面同步的`Log`应用于状态机（`commit`），可`Leader`何时会告知`Follower`节点可以提交日志呢？为了更好的说明该问题，下面依旧基于之前的动画网站，模拟演示`Raft`集群同步日志的全流程。

### 3.2、Raft日志复制动画过程

![Raft集群](Raft协议.assets/8a85f9ac594d4122b00190ac319bad39~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

上图是一个刚启动的`Raft`集群，目前的`Leader`节点为`S5`，首先我们通过`request`模拟客户端发起操作：

![客户端请求Leader](Raft协议.assets/96e6e3d134b340a0a069f6bf189ae171~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

当请求到达`S5`后，大家会发现右侧的表格多出了一个蓝色方块，该方块则应着一个`Log`，中间的`2`代表当前集群的`Term`，为此，此日志可以表示为`term2、index1`。观察下来，会发现该方块位于`S5`这一栏，这代表着此日志已加入到`S5`的日志序列，按照之前讲述的流程，接下来`S5`会向其余节点发起`AppendEntries RPC`：

![RPC抵达S3](Raft协议.assets/5d17c1a759b046208c1a9903c8c59a23~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

`S3`率先收到了追加日志条目的`RPC`，于是，`S3`也会将对应的日志加入到自己的日志序列里，接着给`S5`返回响应，随着时间推移，其他节点也会陆续收到`RPC`，将日志追加到各自的序列并返回响应：

![响应RPC](Raft协议.assets/099741effbbf476ba84f9aedcb9bb27c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

上图中，`S1~S4`节点都已成功响应`S5`，可值得注意的是，**此时图中五个蓝色方块，其边框都为黑色虚线，意味着本次日志只是追加到了日志序列，并未应用于状态机（未`Commit`）**！何时会提交呢？

![Leader提交](Raft协议.assets/c5de1427d7a84cf2864b806f0a87e3b6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

如上图所示，`S1、S2`的响应还未抵达`S5`，可`S3、S4`的响应已经被`S5`收到了，`S3、S4`再加上`S5`自身，数量已经满足集群大多数的要求，此时身为领导者的`S5`就能确认：**本次客户端操作对应的日志条目已成功复制给大多数节点，本条日志可以应用状态机**，所以，图中`S5`对应方块边框，变成了黑色实线，意味着`S5`上的日志已提交。

从这里就能看到前面说的问题，`Leader`上提交日志后，`S3、S4`并未提交，何时提交呢？

![S5心跳](Raft协议.assets/59c5fdf26ee442af8d7ffe28e07b0091~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

在《领导者选举》章节提到过，`Leader`为了维持其领导者地位，正常运行期间会不断发出心跳包，而在`Leader`发出的心跳`RPC`中，就会塞进一个额外的信息：**`Leader`上已应用于状态机的日志索引，即`S5`节点已提交（`Committed`）的日志索引**，图中`S5`发出的心跳包，其`AppendEntriesRPC`对应的伪结构体如下：

```java
public class AppendEntriesBody {
    // 任期编号
    private Integer termId;
    
    // 提交的日志索引（下标）
    private Integer commitIndex;
    
    // 省略其他字段……
}
```

当然，实际`AppendEntriesRPC`的结构体并不会这么简单，具体会在后面的章节进行展开。好了，继续往下看：

![响应心跳](Raft协议.assets/5e774d0561e945e69d875cd80540a3ae~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

当`S5`发出的心跳包，被`S1~S4`节点收到后，`S1~S4`会对比`RPC`中携带的`commitIndex`，这时就能得知前面复制的日志，究竟能否应用于状态机。案例中，`S5`心跳包的数据可以简述为`term2、commitIndex1`，`S1~S4`发现`Leader`已经提交了索引为`1`的日志，同样会陆续提交前面追加到各自序列中的日志（图中各节点的方块边框都变为了实线）。

综上，我们完整阐述了`Raft`日志复制的完整流程，也解答了上面提出的疑惑，简单总结下，**其实整体流程有点类似于两阶段提交，第一阶段将客户端的操作先封装成`Log`，而后同步给所有节点并追加到序列；第二阶段再让所有`Follower`节点`Apply`前面复制的日志**。

### 3.3、Raft的一致性保证

上阶段通过动画演示了`Raft`日志复制的全流程，通过这套机制，在集群通信正常、所有节点不出意外的前提下，就能保证所有节点最终的的日志序列与状态机一致且完整。在此基础上，**`Raft`保证不同节点的日志序列中、`term，index`相同的日志，存储的操作指令也完全相同**，即`S1`节点的`term1，index1`存储着`x←1`，`S2、S3`节点的日志序列，相同位置一定也存储着该指令。

> PS：`Raft`协议中，`Leader`针对同一个`Index`只能创建一条日志，并且永远不允许修改，意味着`term+index`可以形成“全局唯一”的特性。

不过网络总是不可靠的，不同节点间的网络状况不同，任意时刻、任意节点都可能会发生延迟、丢包、故障、分区、乱序等问题，来看个抖动重发造成的乱序场景：

![重发乱序](Raft协议.assets/20442928284f4cfcb795a4fbebb7b8d9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

上图中，客户端先向身为`Leader`的`S2`节点，发出了`x←1`的操作，接着又发出了一个`x←2`的操作，按照前面的推导，`S2`会按先后顺序、几乎“同时”处理者两个客户端操作（因为`Raft`支持多决策），接着向其他节点同步对应的日志，假设`S2`和`S5`之间网络出现抖动，导致`x←1`对应的日志重发，这意味着`x←2`会比`x←1`的日志先抵达`S5`。

此时注意观察上图中各节点的日志序列，`S5`的日志序列就与其他节点的日志序列存在差异，其`index`为`2`的位置，存放的是`x←1`，这代表最终应用到状态机里的`x=1`！这时，当客户端从集群读取`x`，`S1~S4`读到的为`2`，而`S5`读到的则为`1`，造成数据的不一致现象。

为了解决这类问题，**`Raft`要求`Leader`在发出`AppendEntries-RPC`时，需要额外附带上一条日志的`term，index`，如果`Follower`收到`RPC`后，在本地找不到相同的`term，index`，则会拒绝接收这次`RPC`**！套进前面的例子再来分析。

`S2`先处理`x←1`操作，在此之前没有日志，所以上一条日志的信息为空，对应的`RPC`伪信息如下：

```json
{
    "previousTerm": null,
    "previousIndex": null,
    ……
}
```

接着`S2`处理`x←2`操作，`Leader`对应的序列中，在此操作之前有个`x←1`，所以对应的`RPC`类似这样：

```json
{
    "previousTerm": 2,
    "previousIndex": 1,
    ……
}
```

在这种情况下，如果`S5`先收到`x←2`对应的`RPC`，在本地序列找不到`term2，index1`这个日志，就能得知该日志顺序不对，为此会拒绝掉本次`RPC`，并返回自己需要的日志（`term2，index1`）。等到`x←1`对应的日志抵达后，才会接受对应追加日志条目的`PRC`。

> PS：这种机制被成为一致性检查机制，也可以用来帮助掉线恢复后的节点，补全断线期间错过的日志（细节会在后续展开）。

通过这种机制，只要`Follower`没有陷入故障状态，通过不断归纳验证，就一定能和`Leader`的日志序列保持一致，因此，`Raft`也能保证：**不同节点日志序列的某个日志（`term，index`）相同，那么在此之前的所有日志也全部相同**，比如`S3`的`term2、index78`是`a←5`，`S2`也有`term2、index78`这个日志，那么它们的值肯定一样，并且前面`77`个日志也完全相同！

## 四、日志复制的一致性隐患

`Raft`通过一致性检查，能在一定程度上保证集群的一致性，但无法保证所有情况下的一致性，毕竟分布式系统各种故障层出不穷，如何在有可能发生各类故障的分布式系统保证集群一致性，这才是`Raft`等一致性算法要真正解决的问题，来看`Raft`论文中给出的经典案例：

![日志混乱](Raft协议.assets/2521021dc6e74c139692fe194980f730~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

上图展示了第八个任期中，新`Leader`刚上任的集群情况，一眼望过去，大家会发现集群的日志序列混乱不堪，最上面的则的`Leader`的日志序列，而下面则列举了六种`Leader`上线可能遇到的混乱场景，其中有的多了部分日志，有的少了部分日志，为什么会造成这些现象呢？下面来逐个分析下。

> PS：上图并不是一个集群，而是列出了六种`Leader`上线有可能遇到的混乱场景！

### 4.1、日志不一致场景分析

**情况一：a、b比Leader少了一部分日志**。这种现象经过前面的内容讲解后，其实很容易观察出问题，即`Follower-a`在收到`term6，index9`日志后掉线，`Follower-b`节点在收到`term4，index4`后掉线，从而导致两种日志落后于新`Leader`的场景出现。

**情况二：c比Leader多了一个term6中的日志**。造成这种情况的原因，是由于`term6`的`leader`，刚向`Follower-c`发出`term6，index11`的日志，还没来得及给其他节点同步就发生了故障，然后开启了`term7`的选举，`term7`这轮任期，有可能没选出`Leader`，又或者`leader`刚上线没多久就挂了，所以图中的`leader`才会直接成为`term8`的领导者。

**情况三：d比Leader多了term7的日志**。图中的`d`比`leader`多出两个`term7`的日志，这种情况也很好解释，即`term7`的`leader`节点刚上线没多久，只将`term7，index11`、`term7，index12`两条日志同步给了`Follower-d`，然后就挂掉了，此时就造就了`d`场景出现。

**情况四：e比Leader多了两个term4的日志，少了term5、term6的日志**。这种情况是`term4`的领导者，在提交完`term4，index4~5`日志后，刚将`term4.index6~7`日志同步给`Follower-e`，接着就挂掉了，因此`e`比`term8`的`leader`多两个`term4`的日志。同时，`e`在收到`term4，index6~7`两个日志后也掉线了，所以`term5~6`的日志全都未同步。最后，图中`leader`身处`term8`，意味着`term7`没选出`leader`，或`term7`的领导者在任时间很短暂。

**情况五：f比leader多了term2-3的日志，少了term4-6的日志**。`f`拥有的`term2~3`日志在`leader`上不存在，这就只能说明`term2、3`两个任期中，原`leader`只将日志同步给了`Follower-f`，然后就掉线了，而`f`在收到`term3，index11`这条日志后，也发生了故障从而掉线，再次恢复时，集群已经推进到`term8`这个任期。为此，`f`才会多出`term2~3`、缺少`term4~6`的日志。

好了，上面捋清楚了`Raft`论文中列出的五种混乱现象，造成这些现象的原因很简单，因为现实场景中节点发生故障的时间不可控，任意时间某个`Follower`或`Leader`掉线后，都会导致其日志序列与其他节点脱节，再次恢复后，其日志序列就会和最新的`Leader`存在差异。那么，`Raft`协议究竟是如何解决这么多种复杂场景的呢？

### 4.2、Raft如何解决一致性隐患？

其实`Raft`解决问题的手段很简单粗暴，在之前说过：**Raft是一种强Leader的一致性协议，一旦某个节点成为Leader，那么在其任职期间，它将拥有集群中至高无上的权力**！正因如此，当集群节点出现日志序列不一致问题时，**`Raft`会强制要求存在不一致的`Follower`节点，直接复制在任`Leader`的日志序列来保持一致性**！

> 简单来说，就是当新`Leader`上任后，发现集群存在与自身序列不一致的`Follower`节点时，会使用自身序列中的日志，覆盖掉`Follower`节点中不一致的日志（`Leader`从不会丢弃自己序列里的日志）。当然，`Leader`是如何发现集群中存在不一致的`Follower`呢？大家还记得上面聊的一致性检查机制嘛？

`Leader`在每次发出`AppendEntries-RPC`时，都会携带自身上一个日志的任期号、日志下标，如果集群里存在不一致的`Follower`节点，在接受`PRC`必然无法通过一致性检查，通过这种机制，就能确认集群各节点的日志是否与自身一致。

> 同时，`Leader`针对每个`Follower`节点，都会维护一个索引，即`Next-Index`，从其命名也能轻易得知其作用，就是用来记录下一个要发往`Follower`的日志下标，在`Leader`刚上任时，`Next-Index`默认为自己最后一个日志的下标加一。

结合前面聊到的一致性检查机制，当集群存在不一致的`Follower`时，`Leader`发出的`AppendEntries-RPC`就无法通过一致性检查，此时`Leader`上维护的对应`Next-Index`会减一，经过不断归纳验证，总能找到两者最后达成一致的日志位置，接着会将之后所有不一致的`Log`全部覆盖。当然，碰到极端场景，`Next-Index`可能会变成`1`，即`Follower`上的所有日志都需要重新复制。

## 五、Raft安全性（Safety）

上面的内容讲述了领导者选举和日志复制两个核心问题，但仅靠这两方面并不能保证集群的正确性，在很多情况下，集群仍然存在不一致风险！比如上阶段末尾提到的一致性隐患问题，尽管`Raft`通过强`Leader`特性，结合一致性检查机制，解决了提到的多种混乱场景，可这种方案真的万无一失吗？就目前而言并不够，因为到目前为止，当`Leader`掉线后，只要任意节点得到了大多数节点的投票支持，就有可能成为集群的新`Leader`，如下图所示：

![选举风险](Raft协议.assets/b88de148213545208a78508fcd28a357~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

目前集群`Leader`是`S1`，假设突然掉线，如果`S3`率先感知到，根据之前描述的选举机制，`S3`则有最大概率成为新`Leader`，此时问题来了，`S3`的日志明显落后于其他节点，一旦它成为新主，结合刚才所说的一致性方案，就会造成所有节点的日志回退到`index4`这个位置（因为`Leader`会覆盖掉`Follower`上不一致的日志）。

上面所说的这种情况，显然并不合理，**因为之前的日志已经提交到`index11`，一旦`S3`上任将之前的日志覆盖，就会导致客户端读不到之前已经写入成功的数据**，这对集群而言，无疑是一种不可容忍的错误。为此，想要保证集群的正确性，在做领导者选举时，必须得加些额外限制，而这就是接下来要聊到的安全性保障！

### 5.1、选举机制的安全性保障

仔细分析前面的问题，其实会导致集群发生不可逆转的错误，根本原因在于：**没有设立成为新Leader的门槛**，类比到生活，如果你部门的一把手离职，一个刚入职的应届生能否坐上他的位置呢？显然不能，因为想要坐上那个位子，有着一系列隐形门槛，如经验、为人处世等，而想要解决上面提到的问题，选举时加个限制即可。

和之前一样，率先感知到`Leader`掉线的节点，依旧能最快成为候选者，但它能否成为新一轮的`Leader`，这并不能保证，因为`Raft`对选举加了一条安全性限制：**Candidate发出RequestVote-RPC拉票时，必须携带自己本地序列中最新的日志（term，index），当其他Follower收到对应的拉票请求时，对比其携带的日志，如果发现该日志还没有自己的新，则会拒绝给该候选人投票**。

将这个门槛套进前面的例子中，当`S3`感知到`S1`掉线后，尽管它最先发起拉票，可`S2、S4、S5`的日志都比它要新，所以不会有任何节点给它投票，就无法满足“大多数”这个条件，`S3`就必然无法成为新`Leader`。反之，**如果一个节点成为了新`Leader`，那么它一定得到了集群大多数节点的支持，也就意味着它的日志一定不落后于大多数节点**。

> 对比的规则：**如果任期号（`term`）不同，任期号越大的日志越新；如果任期号相同。日志号（`index`）越大的越新**。

OK，再来看前面的例子，如果不考虑超时机制的情况下，谁最有可能成为新`Leader`？显然是`S5`节点，因为它具备最全的日志！那再来看个问题，如果`S3`拉票失败后，`S4`率先超时，此时它发起拉票请求，任期被推进到`7`（`S3`虽然拉票失败，但它自增的任期会保留），`S4`能否有机会成为新`Leader`呢？如下：

![S4拉票](Raft协议.assets/d3117143c1ef48c892cb8e11b757dbd7~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

此时来看，`S4`开始向其余节点拉票时，自身最新的日志为`term5，index11`，各节点的回应如下：

- `S1`已掉线，不会响应拉票请求；
- `S2`本地最新的日志为`term4, index8`，会投一票；
- `S3`本地最新的日志为`term2, index4`，会投一票；
- `S5`本地最新的日志为`term5, index12`，会拒绝投票。

此时来看，尽管拥有最新日志的`S5`节点拒绝投票，可`S2、S3`节点的两票，再加上`S4`自身的一票，依旧能让`S4`的票数满足“大多数”这个条件，为此，`S4`毋庸置疑会成为`term7`的新`Leader`。有人或许会疑惑，`S5`比`S4`日志要新呀，如果`S4`当选`Leader`，和`S5`节点是不是存在一致性冲突呀？没错，可是这并不影响集群的一致性，`Why`？大家仔细来看这张图：

![已提交的日志](Raft协议.assets/d08233750c874109ad2fa8922c8ef658~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

其实身为原`Leader`的`S1`掉线时，集群内日志只提交到了`index11`这个位置，而`S5`节点多出来的`index12`这条日志，实则并未被提交，因为它并未被复制到大多数节点，所以`S1`也不会向客户端返回“操作成功”，这意味什么？**意味着`S1`节点的`term5，index12`这条日志可以被丢弃，即使后续被覆盖了，也并不会影响客户端的“观感”**，毕竟这条日志对应的`b←2`操作，在客户端的视角里，本来就没写入成功。

好了，说到这里大家应该也明白了，为什么封装的客户端操作日志，至少要在集群大多数节点同步完成后，才能向客户端返回操作成功的根本原因！**就是因为在做领导者选举时，只要一条日志被复制到了大多数节点，那么这些已提交的日志，在选举出来的新Leader上就一定存在，这也是Raft为何能保证日志一旦提交，就一定会被`Apply`到状态机、且永远不会丢失的原因**。

### 5.2、日志复制的安全性保障

上小节讲到了`Raft`对领导者选举增加的安全性限制，可如若你觉得已经彻底解决了一致性问题，那就大错特错！再来看一个`Raft`论文中给出的经典问题：

![日志提交带来的不一致冲突](Raft协议.assets/aed78ae96c714da68677867b937e24d6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

上图描述了一个日志提交带来的一致性冲突问题，图中是由五个节点组成的集群，并且所有节点已具备`term1，index1`这条日志，而后从左到右按时间顺序描述了问题的背景，分为五个阶段。

阶段`a`中，`S1`是集群的`Leader`，此时客户端发来了一个操作，`S1`将其封装为对应的日志（`term2.index2`），可是刚复制给`S2`后，`S1`发生故障从而掉线。

阶段`b`中，`S5`率先超时，并获得`S3、S4、S5`的三票，成为`term3`的新`Leader`，然后客户端也发来了一个操作，`S5`刚将其封装成`term3，index2`这条日志，还未来得及同步，就发生了故障。

阶段`c`中，`S1`已经恢复，且最先超时，`S1`获得`S1、S2、S3、S4`四票，重新成为`term4`的`Leader`，于是继续向`S3`复制之前的`term2，index2`日志，`S3`复制成功，此时这条日志满足了“大多数”条件，`S1`将其提交（`commit`）。

阶段`d`中，`S1`又掉线，`S5`恢复并携带着自身最新的日志（`term3，index2`）开始拉票，根据之前的对比原则，任期号越大日志越新，所以`S5`能获得`S2、S3、S4、S5`四票，从而再次成为`term5`的`Leader`。这时，`S5`将`term3，index2`复制给所有节点并提交。

大家注意看，在阶段`d`中，`S5`将`term3，index2`复制给所有节点时，其实在此之前，`term4`中的`S1`，已经将`term2.index2`提交了，因为阶段`c`时，这条日志已经在`S1、S2、S3`完成复制。而阶段`d`的`Leader`是`S5`，根据之前的原则：**Leader在当前阶段中拥有最高的权力，有权覆盖掉与自身不一致的日志**，为此，`term2，index2`就会被`term3，index2`覆盖。

问题就出在这里，`term2.index2`已经被提交，对客户端而言是可见的，可是到了阶段`d`，这条日志又被覆盖，最终又给集群带来了无法容忍的致命错误！怎么解决呢？`Raft`仅对日志提交加了一个小小的限制：**Leader只允许提交（Commit）包含当前任期的日志**。

值得注意的是，**这条限制里说的是“只允许提交包含当前任期的日志”，而不是“只允许提交当前任期的日志”**！啥意思？套进前面的例子中，导致不一致问题出现的时间为阶段`c`，因为此阶段对应的任期为`term4`，可是却提交了`term2，index2`这个第二轮任期的日志，所以造成了不一致冲突。

可之前又提交过，`Leader`永远不会丢弃自身的日志，那么`term2，index2`这条日志什么时候会被提交呢？**需要等到`S1`收到`term4`的操作后，并将其封装成日志复制到大多数节点时，与`term4`的日志一起提交**。通过这条限制，`term2，index2`就会跟随`term4`的日志一起提交，如果`S1`担任`term4`的领导者期间，并未出现任何一条客户端操作，那么`term2，index2`就永远不会被提交。

好了，结合上述限制，再来看到阶段`e`，如果`S1`任职的`term4`中出现了新的客户端操作，那么`term2，index2`会随着`term4，index3`这条一同被提交，这时就算`S1`掉线，`S5`也无法成为新的`Leader`，因为`S2、S3`的日志都比它新，所以`S5`永远无法满足“大多数”这个选举条件。

> 反之，如果`term4`中没有客户端操作到来，`term2，index2`就不会被提交，这时担任`term5`领导者的`S5`，就算将这条日志覆盖，也不会对客户端造成不一致的观感，因为未提交的日志不会应用于状态机。

### 5.3、领导者选举时的细节问题

好了，前面已经将`Raft`分解出的领导者选举、日志复制、安全性这三个子问题阐述完毕，下面来讨论一个选举时的细节问题：

> 目前由`S1、S2、S3、S4、S5`五个节点组成集群，现任`Leader`是`S5`，`S5`如果在发出心跳后，由于`S2`节点网络较差，导致接收心跳包出现延迟，从而造成`S2`的随机选举时间出现超时，然后发起一轮新选举怎么办？

这时`S5`是否会被`S2`替换掉呢？这个问题要结合前面所有知识来分析，因为发起新一轮选举会自增任期号，而`Follower`在投票时，如果发现对方的任期号要比自身大，且日志不小于自身最新的日志，就会为其投票。假设这时`S2`具备最新的日志，尽管原本身为`Leader`的`S5`节点很正常，`S2`也会成为新`Leader`。

有人或许会说，在这种情况下，是`S2、S5`之间的网络存在波动、不稳定导致的，`S2`就将正常的`S5`节点挤下线，这太不公平了！的确有点不公平，但却无伤大雅，毕竟作为新`Leader`的`S2`，也具备完整的已提交日志，并不会影响集群正常运行。

> 同时，如果是`S2`自身的网络一直存在问题，比如网络带宽延迟较高，网络传输速度不够稳定等等，那么它肯定不会有机会当选`Leader`！为啥？**因为网络存在问题的节点，永远不可能具备最新的日志**，毕竟`Raft`也是基于网络来发送`RPC`，如果一个节点网络本身有问题，那么其同步日志的效率必然很缓慢。

## 六、Raft日志压缩机制

前面已将`Raft`算法的核心内容阐述完毕，但如果想要将其应用实际的工程中，那么还需要考虑一些现实因素带来的问题，首先来看看日志增长的问题。因为`Raft`是基于日志复制工作的一致性算法，并且该算法主要服务于分布式存储的集群领域，任何一款分布式存储组件，一旦将其部署后，持续运行的时间必然不短。

也正因如此，`Raft`要面临的并非单次、几次一致性决策，而是数以几百万、几千万，甚至几十亿次决策，在上面讲述的内容中，`Raft`为了让一个客户端的操作，在集群内达成一致，会先由`Leader`将其封装成日志条目，接着同步给其余节点。那么，我们可以将这个关系简单描述为：**一次决策等于一条日志**。

既然集群的长时间运行会触发无数次决策，这代表着对应的日志会呈现无上限式增长，而现实中的硬件设施并不支持这么做，毕竟一台机器的存储空间再大，也总会有被存满的一天，**无限制的日志增长，会占用不可预估的存储空间**。其次，`Raft`状态机依赖于日志，**这就意味着当机器重启时，又或者新的节点加入集群，需要重放之前的所有日志，才能将拥有集群最新的数据**，这无疑会极大程度上拖慢集群的可用性。

综上，如何控制日志的无限增长，这成为了`Raft`在工程实践中第一道坎，而这个问题也是许多分布式存储组件面临的问题，对应成熟的解决方案叫做：**日志压缩技术**。

### 6.1、什么是日志压缩？

压缩技术相信大家都有所接触，日常传输一个文件时，如果源文件较大，必然会影响传输效率，为了缩短传输时间，大家都会将其打成`.zip、.rar、.tar`等格式的压缩包。同理，这个思想也可以用于`Raft`中，当日志序列较大时，我们可以通过压缩技术对其进下瘦身工作。

可是，传统的压缩技术并不能解决`Raft`所面临的难题，因为传统的压缩技术，最多只能在原大小的基础上“瘦身”`30~40%`，这对无限增长的日志而言用处不大。因此，该如何有效解决客户端持续性操作，带来的日志无限增长问题呢？答案很简单，**依靠`Snapshot`快照技术**。

`Snapshot`快照技术是编程领域最常用、最简单的日志压缩机制，`Zookeeper、Redis`底层都有用到此技术。**快照就是将系统某一时刻的状态`Dump`下来，在此之前的所有操作日志都可以舍弃**。这是啥意思呢？来看个例子：

![快照机制](Raft协议.assets/a530947735a54bacaacea9bbb9a204d5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

上图左边是四条日志，而右边则是这四条日志对应的快照文件，很明显，诸位会发现快照比日志“小”了许多，为什么呢？因为左边的四条日志，依次`Apply`于状态机后，得到的最终结果就是`x=5`，所以，我们只需要保留最终的结果，从而达到缓解无限制增长带来的存储压力。

> 注意：我们可以把日志序列压缩成一个快照文件，但却无法根据快照文件提取出原本的日志序列。

### 6.2、Raft快照技术

经过上小节讲述大家会发现，**所谓的快照技术，就是“省略过程，保留结果”的产物**，当然，因为客户端操作会一直持续，因此，只要系统还在运行，就始终无法得到一个永久有效的快照文件，为了尽可能减小日志增长带来的额外空间压力，我们需要定期保留系统某个时刻的快照，再来看个例子：

![Raft快照](Raft协议.assets/3dfdc56498ce459eb6a5c52c1609c03e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

这是一个包含多任期的日志序列，上图描述了`term2~5、index1~12`转换出的快照文件，实际上就是“当下时刻”的状态机。如果对[Redis-AOF日志重写机制](https://juejin.cn/post/7097521572885299214#heading-8)较为熟悉的小伙伴，看这个例子同样会异常亲切，毕竟它两之间有着异曲同工之妙。`Redis`的`AOF`日志，记录着自启动后、运行期间内收到的所有客户端操作指令，为了有效解决无限制增长的难题，当`AOF`文件大小达到一定阈值后，`Redis`会对其进行重写。

重写`AOF`日志，就是对其进行一次压缩，重写动作发生时，会先生成当下时刻的内存快照，而后将快照中的每个数据，反向生成出每个数据的写入指令，接着不断追加到新的`AOF`文件中，当快照文件全部被转换为`AOF`指令后，最后就能用新的`AOF`覆盖原本体积较大的`AOF`文件。

`Raft`日志压缩亦是同理，但`Raft`并不会用快照反向生成日志序列，而是只留存快照文件、丢弃生成快照之前的日志，所以，一个快照文件中包含两种信息：

- ①生成快照时，当前`Leader`节点的状态机（数据）；
- ②生成快照时，最后一条被应用于状态机的日志元数据（`term、index`）。

第一种信息比较好理解，而第二种信息主要是为了兼容原有的日志复制功能，比如当一个新加入的节点，需要很早同步之前的日志，这时可能已经被压缩成了快照文件，就可以根据快照的日志元术据来判断，如果该节点需要的日志，要老于生成快照时，最后一条被`Apply`到状态机的日志，这时`Leader`可以把整个快照发给新节点（这种方式还能减少重放日志带来的耗时）。

> PS：同步快照并不是通过`AppendEntries-RPC`完成，而是通过另一种新的`InstallSnapshot-RPC`来实现。

最后，由于运行期间内会不断生成新的快照，**而每当生成一个新的快照文件时，在之前的老快照文件都可以被舍弃，因为新的快照文件总能兼容旧的快照文件**，如果新快照比旧快照少了部分数据，这只能说明两次快照间隔期间，客户端出现“删除”操作，因此少的那部分数据也并不重要。

## 七、Raft动态伸缩机制

聊完日志压缩技术后，下面来看看另一个较为核心的问题，**即集群成员变更机制**，一套系统部署后，没有人能保证部署这套系统的机器一直正常，在现实场景中，往往会因为诸多因素，造成集群成员出现变更，比如原本集群中的`A`节点，因为所在的机器硬件设施太落后了，所以有一天想要使用配置更优的`D`来取代它，这就是一种典型的集群成员变更。

除上述情况外，在如今云技术横行的时代，为了拥抱各种不定性的业务场景，许多云平台都提供了弹性扩容、动态伸缩等机制的支持，那如果一种采用`Raft`的技术部署在云环境中，由于业务访问量突然暴增，触发了云平台的弹性扩容机制，将原本的`3`节点规模，提升到`5`节点规模，这时就会多出两个新节点，而这也是一种成员变更的情况（节点收缩亦是同理）。

正如上面所说，`Raft`想要真正在工程中实践，如果不去考虑成员变更的问题，那就只能如“旧时代”的模式一样，一旦集群要发生成员变更，就先停止整个集群，接着人工介入完成成员变更，最后重新启动整个集群。这种方式很简单，不过最大的问题是：**系统在变更期间必定不能对外服务**，这个苛刻的条件对许多大型系统而言是无法容忍的。

怎么办？不用担心，`Raft`的作者也想到的这个问题，因此在论文中也给出了一种运行期间内、自动完成成员变更的机制，**也就是将集群成员变更的信息，也封装成一种特殊的日志（`Configuration Log Entry`），再由`Leader`同步给集群原本的其他节点**，下面来展开聊聊。

### 7.1、集群成员变更造成的脑裂问题

在聊`Raft`提供的成员变更机制之前，我们先来看看集群中经典的脑裂问题，**所谓脑裂，即是指中心化的集群中，同一时刻出现了两个`Leader/Master`节点**，脑裂问题通常发生于网络分区场景中，而集群成员变更则是最容易导致网络分区产生的一类场景，来看具体例子：

![Raft成员变更](Raft协议.assets/6d0232f43137408a9e3912640140421d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

上图是`Raft`论文中给出的一张集群成员变更图，不过较为抽象，有点难以让人理解，所以我们可以将其拆解为如下四个阶段：

![集群成员变更](Raft协议.assets/f060f927af8a46f9b737285b63566422~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

上图演示了对三个节点组成的集群，动态扩容两个节点后遇到的成员变更情况，其中也逐步说明了脑裂问题的产生，我们依旧按时间顺序，从左到右挨个讲解。

**在第一阶段中**，集群由`S1、S2、S3`三个节点组成集群，其中`S3`为现阶段的`Leader`，当然，在`Raft`论文中，这组配置被称之为`C-old`，代表老集群的节点配置。

**到了第二阶段**，集群扩容`S4、S5`两个节点，集群出现成员变更场景，此次变更仅告知给了身为领导者的`S3`节点，再由`S3`同步给原集群中的`S1、S2`两个节点（目前集群变成`S1~S5`五个节点组成，这组新的节点配置称为`C-new`）。

**来到第三阶段**，此时`S3`还未来得及将`S4、S5`已加入集群的消息同步给`S1、S2`，`S3`就突然出现短暂的故障（如网络不可用），导致其未及时向集群所有节点发送心跳，最终引发`S1、S5`两个节点的超时，`S1、S5`各自发起新一轮选举。

**在第四阶段里**，因为`S1`还不知道集群节点已经增加到了五个（S3未来得及告知），所以它只会向`S2、S3`节点拉票，又因为`S3`是原本的主节点，`S1`的日志肯定不可能比`S3`要新，因此`S3`会拒绝给`S1`投票，而`S2`会投出自己的一票，此时再加上`S1`持有自身的一票，顺理成章当选新`Leader`。

而`S5`作为新加入的节点，它已知现在集群里有五个节点，所以会同时向`S1~S4`发起拉票，因为`S5`和`S3`具备相同的日志（`S3`宕机前的最后一条日志，就是`S4、S5`加入集群），所以`S3`会将自己的一票投给`S5`，而作为一同加入集群的`S4`节点，也必然会将票数投给`S5`，此时加上`S5`自身的一票，总共获得三票，满足“大多数”这个选举条件，最终`S5`也会宣告自己是新`Leader`。

经过第四个阶段后，大家会发现此时集群中出现了`S1、S5`两个`Leader`，这就是经典的脑裂问题，也是任何主从复制集群零容忍的致命错误。到这里，我们讲述清楚了脑裂问题的产生背景，那`Raft`中是如何解决这个致命错误的呢？

### 7.2、Raft的联合共识变更机制

首先记住，**因为成员变更也需要依靠日志同步机制，来告知给所有的Follower节点**，而日志同步需要借助网络发送RPC，所以旧集群中的所有节点，不可能在同一时刻共同感知集群成员发生了变更。正因如此，在`Follower`同步成员变更日志这个期间，就可能会存在“不同节点看到的集群配置（视图）不一样”的情况，如果这期间`Leader`发生故障，或许就会引发脑裂情况发生。

`Raft`论文中表明：**任何直接将集群从`C-old`（旧配置）直接切换成`C-new`新配置的方式都不可靠**，即直接切换都有可能导致脑裂现象，为此，`Raft`提出一种两阶段式的成员变更机制，这种机制在论文中被称为：**联合共识（Joint Consensus）策略**，该策略对应的两个阶段为：

- 阶段一：由`Leader`先将发生集群成员变更的消息通知给所有节点；
- 阶段二：等大多数节点都收到成员变更的消息后，再正式切换到新的集群配置。

先来细说一下阶段一中的具体流程：

- ①客户端触发成员变更动作，先将`C-new`发给`Leader`节点，`Leader`在`C-old、C-new`两组配置中取并集，表示为`C-old,new`；
- ②`Leader`将新旧两组集群配置的并集`C-old,new`，封装成特殊的日志同步给所有`Follower`节点；
- ③当大多数`Follower`收到并集后，`Leader`将该并集对应的日志提交。

这是第一个阶段的流程，稍微说明一下其中的并集概念：

> 并集：由两个或多个集合之间，所有非重复元素所组成的集合；

比如`{A,B,C}`与`{D,E}`的并集就为`{A,B,C,D,E}`，而`{A,B,C}`与`{A,C,D}`的并集则为`{A,B,C,D}`，在`Raft`算法中，`C-old、C-new`这两组节点配置可以视为两个集合，两者的并集则被表示为`C-old,new`。

接着来聊下阶段二的详细流程：

- ①`C-old,new`的日志提交后，`Leader`继续将`C-new`封装为日志同步给所有`Follower`节点；
- ②一个`Follower`收到`C-new`后，如果发现自己不在`C-new`集合中，就主动从集群中退出；
- ③当大多数节点都将`C-new`同步完成后，代表集群正式切换到新配置，`Leader`向客户端返回变更成功。

注意看这个过程，在大多数节点收到并集的日志后，`Leader`就会着手将集群切换到新的节点配置，主要看第二步操作，**如果一个Follower收到C-new日志后，发现自己并不在节点列表中**，这就说明本次成员变更，自己就是要被替换掉的一员，因此当前节点就需要从集群主动退出，从而让集群从旧配置切换到新配置。

![联合共识](Raft协议.assets/6939c322d74a4d249646d8a2cd4ff471~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp)

这仍是一张摘自`Raft`论文的原图，其中描述了整个`Joint Consensus`的过程，我们再来分析下基于联合共识实现成员变更后，是否还存在脑裂问题。

### 7.3、Joint Consensus为什么能避免脑裂？

大家一起分析下，在整个联合共识的过程中，集群`Leader`在哪些时间点可能掉线呢？

- ①`Leader`收到客户端的成员变更操作后，还未取得并集就掉线；
- ②并集`C-old,new`的日志还未提交，`Leader`掉线；
- ③并集`C-old,new`的日志已提交，`C-new`还未封装成日志，`Leader`掉线；
- ④`C-new`还未提交，即日志只在少数节点完成同步，`Leader`掉线；
- ⑤`C-new`在大多数节点同步成功，日志已提交，`Leader`掉线。

上述列出了集群成员变更时，所有`Leader`可能掉线的时间节点，接着对其逐个分析下。

先来看第一种情况，因为`Leader`刚收到成员变更操作，都还未来得及取新旧两组配置的并集就挂了，这时集群所有`Follower`节点必然都只能看到`C-old`配置，所以这种情况不可能选出两个新`Leader`。

> 说明：所谓的“看到”，就是指某个节点所身处的配置，比如`A`能看到`C-old`，代表`A`在老的集群配置中。

再看第二种情况，并集`C-old,new`的日志还未提交，意味着`Leader`已经提取出了新旧配置的并集，并且封装成`C-old,new`日志并开始向其他节点同步，但日志还未提交，说明集群内只有少数节点同步成功，等价于集群内有少数节点能看到`C-old,new`这组并集配置，还有大多数节点只能看到`C-old`旧配置。可不管并集配置也好，旧配置也罢，实则都包含`C-old`这组旧节点列表，意味着任何一个节点想成为新`Leader`，必须获得`C-old`中大多数节点的认可。

> 一个新加入集群的节点，能否拿到`C-old`里的大多数投票呢？答案是不能，因为`C-old,new`日志还未提交，这意味着集群大多数`Follower`节点，并不知道有新节点加入，那它们必然不会将票数投给新加入的节点，因此，新加入集群的节点肯定没机会在这种情况下成为新`Leader`。

接着看到第三种情况，`C-old,new`已提交、`C-new`未封装成日志，这代表集群大多数节点都能看到`C-old,new`配置，这时`Leader`，一个节点触发超时选举后，能成功变为新`Leader`的节点，肯定已经同步`C-old,new`日志，为什么？**因为大多数节点已同步此日志，没有同步该日志的节点拉票会被拒绝，为此，这种情况同时只会有一个节点拿到`C-old,new`的大多数投票，也只会产生一个`Leader`**。

在第四种情况中，`C-new`还未提交，代表`Leader`已经开始将`C-new`日志同步给其他节点，但只有少数节点完成了同步，接着`Leader`挂掉。这时，如果只能看到`C-old`的节点发起拉票，肯定无法满足大多数，因为目前大多数节点都已经能看到`C-old,new`，这意味着新`Leader`必须要获得大多数`C-new`的票选才能胜任。

最后看到第五种情况，`C-new`日志已经提交，那代表集群已经切换到了新配置，大多数节点都能看到`C-new`，这时`Leader`掉线，新`Leader`也只能从`C-new`里选出来，同一轮任期中，也只会有一个节点拿到`C-new`的大多数投票，自然就不存在多`Leader`出现的场景。

综上所述，`Raft`通过取`C-old、C-new`的并集，来作为成员变更期间的过渡，如果`Leader`在成员变更期间宕机，**根据不同的时间点，上任新`Leader`的节点，需要满足`C-old、C-new`两组配置中的联合共识**，这就是`Raft`中的`Joint Consensus`策略！
