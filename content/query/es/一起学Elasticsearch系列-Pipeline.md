---
title: Pipeline
type: docs
---

在现代的数据处理和分析场景中，数据不仅需要被存储和检索，还需要经过各种复杂的转换、处理和丰富，以满足业务需求和提高数据价值。

Elasticsearch Pipeline作为Elasticsearch中强大而灵活的功能之一，为用户提供了处理数据的机制，可以在数据索引之前或之后应用多种处理步骤，例如数据预处理、转换、清洗、分析等操作。

## 使用场景

Elasticsearch Pipeline 可以用于多种实际场景，其中包括但不限于：

- 数据预处理：对原始数据进行清洗、标准化、去除噪声等操作，保证数据质量和一致性。
- 数据转换：将数据转换为更加符合业务需求的形式，例如字段映射、格式转换、数据合并等。
- 日志处理：实时日志数据的解析、提取关键信息、计算指标、数据聚合等操作。
- 数据安全：对敏感数据进行脱敏处理、数据屏蔽、权限控制等操作，确保数据安全性。

## 具体使用

要实现Elasticsearch Pipeline功能，需要在节点上进行以下设置：

1. **启用Ingest节点**：确保节点上已启用Ingest处理模块（默认情况下，每个节点都是Ingest Node），因为Pipeline是在Ingest处理阶段应用的。可以在elasticsearch.yml配置文件中添加以下设置来启用Ingest节点：

   ```yaml
   node.ingest: true
   ```

2. **配置Pipeline的最大值**：如果需要创建复杂的Pipeline或者包含大量处理步骤的Pipeline，可能需要调整默认的Pipeline容量限制。可以通过以下方式在elasticsearch.yml配置文件中设置Pipeline的最大值：

   ```yaml
   ingest.max_pipelines: 1000
   ```

3. **检查内存和资源使用**：确保节点具有足够的内存和资源来支持Pipeline的运行，避免因为资源不足而导致Pipeline执行失败或性能下降。

对上述参数进行合理的配置后，就可以定义 Pipeline，并将其应用于索引文档了。

下面是一个简单的示例代码，演示如何创建和使用Pipeline：

**创建Pipeline**

```json
PUT _ingest/pipeline/my_pipeline
{
  "description" : "My custom pipeline",
  "processors" : [
    {
      "set": {
        "field": "new_field",
        "value": "example"
      }
    },
    {
      "uppercase": {
        "field": "message"
      }
    }
  ]
}
```

上面的代码定义了一个名为 `my_pipeline` 的Pipeline，包含两个处理步骤：

- `set` 处理器：将字段 `new_field` 设置为固定值 `example`。
- `uppercase` 处理器：将字段 `message` 中的文本转换为大写。

一个Elasticsearch Pipeline通常由以下几个主要部分组成：

- **描述（Description）**：Pipeline的描述部分包含对Pipeline的简要说明或注释，用于帮助其他人理解该Pipeline的作用和功能。
- **处理器（Processors）**：Pipeline的核心是处理器，处理器定义了对文档进行的具体处理步骤。每个处理器都执行特定的操作，例如设置字段值、重命名字段、转换数据、条件判断等。处理器按照在Pipeline中的顺序依次执行，以完成对文档的处理。
- **条件（Conditions）**：可选部分，条件定义了触发Pipeline应用的条件。只有当条件满足时，Pipeline才会被应用到相应的文档上。条件可以基于文档内容、字段值、索引信息等进行判断。
- **内置变量（Built-in Variables）**：在处理器中可以使用一些内置变量来引用文档数据或上下文信息，并在处理过程中进行操作。例如，`_index`表示当前文档所属的索引名称，`_ingest.timestamp`表示处理器执行的时间戳等。
- **标签（Tags）**：可选部分，为Pipeline添加标签，用于标识和分类不同类型的Pipeline。

这些部分共同构成了一个完整的Elasticsearch Pipeline，通过定义和配置这些部分，可以实现对文档数据的灵活处理和转换。

**应用Pipeline**

一旦Pipeline被定义，可以在索引文档时指定应用该Pipeline：

```json
POST my_index/_doc/1?pipeline=my_pipeline
{
  "message": "Hello, World!"
}
```

## 异常处理

在Elasticsearch Pipeline 中处理异常情况通常通过 `on_failure` 处理器来实现。下面是一个示例代码，演示如何使用 `on_failure` 处理器来处理异常情况：

```json
PUT _ingest/pipeline/my_pipeline
{
  "description": "Pipeline with error handling",
  "processors": [
    {
      "set": {
        "field": "new_field",
        "value": "{{field_with_value}}"
      }
    },
    {
      "on_failure": [
        {
          "set": {
            "field": "error_message",
            "value": "{{_ingest.on_failure_message}}"
          }
        }
      ]
    }
  ]
}
```

在上面的示例中，定义了一个名为 `my_pipeline` 的 Pipeline，其中包含两个处理器：

- 第一个处理器使用 `set` 处理器来设置一个新的字段 `new_field` 的值为另一个字段 `field_with_value` 的值。
- 第二个处理器是一个 `on_failure` 处理器，在前一个处理器执行失败时会被触发。这里使用 `on_failure_message` 变量来获取失败的原因，并将其设置到一个新的字段 `error_message` 中。

当第一个处理器执行失败时，第二个处理器会被触发，并将失败信息存储到 `error_message` 字段中，以便后续处理或记录日志。这样可以帮助我们更好地处理异常情况，确保数据处理的稳定性。

如果是Pipeline级别的错误，可以通过全局设置`on_failure`来处理整个Pipeline执行过程中的异常情况：

```json
PUT _ingest/pipeline/my_pipeline
{
  "description": "Pipeline with global error handling",
  "on_failure": [
    {
      "set": {
        "field": "error_message",
        "value": "{{_ingest.on_failure_message}}"
      }
    }
  ],
  "processors": [
    {
      "set": {
        "field": "new_field",
        "value": "{{field_with_value}}"
      }
    }
  ]
}
```

在上述示例中，Pipeline `my_pipeline` 中定义了一个全局的`on_failure`处理器，在整个Pipeline执行过程中发生异常时会触发。当任何处理器执行失败时，全局`on_failure`处理器将被调用，并将失败消息存储到`error_message`字段中。

通过设置全局的`on_failure`处理器，可以统一处理整个Pipeline中任何处理器可能出现的异常情况，提高数据处理的稳定性和可靠性。这样即便是Pipeline级别的错误，也能得到有效的处理和记录，帮助排查问题并保证数据处理流程的正常运行。

## 为索引设置默认Pipeline

从 Elasticsearch 6.5.x 开始，引入了一个名为 index.default_pipeline 的新索引设置。 这仅仅意味着所有摄取的文档都将由默认管道进行预处理：

```json
PUT my_index
{
  "settings": {
    "default_pipeline": "add_last_update_time"
  }
}
```

## 内置Processors

Elasticsearch内置的Processors提供了各种功能，用于在Ingest Pipeline中对文档进行处理。以下是一些常用的内置Processors及其作用：

- **Set Processor**：设置字段的固定值或通过表达式计算值。
- **Grok Processor**：解析文本字段并提取结构化数据。
- **Date Processor**：解析日期字段。
- **Convert Processor**：转换字段类型。
- **Remove Processor**：删除指定字段。
- **Split Processor**：根据分隔符拆分字段。
- **GeoIP Processor**：根据IP地址查找地理位置信息。
- **User Agent Processor**：解析User-Agent字段。

## Pipeline API

，以下是有关Elasticsearch Pipeline API的简要介绍和示例代码：

- **Put Pipeline API**：用于创建或更新Pipeline。

  ```json
  PUT /_ingest/pipeline/my_pipeline
  {
    "description": "My custom pipeline",
    "processors": [
      {
        "set": {
          "field": "new_field",
          "value": "default"
        }
      }
    ]
  }
  ```
  
- **Get Pipeline API**：用于获取Pipeline的信息。

  ```json
  GET /_ingest/pipeline/my_pipeline
  ```
  
- **Delete Pipeline API**：用于删除Pipeline。

  ```json
  DELETE /_ingest/pipeline/my_pipeline
  ```
  
- **Simulate Pipeline API**：用于模拟Pipeline对文档的处理效果。

  ```json
  POST /_ingest/pipeline/_simulate
  {
    "pipeline": {
      "processors": [
        {
          "set": {
            "field": "new_field",
            "value": "default"
          }
        }
      ]
    },
    "docs": [
      {
        "_source": {
          "my_field": "my_value"
        }
      }
    ]
  }
  ```
  
- **Manage Pipelines in Index Templates**：可以在索引模板中定义Pipeline。

  ```json
  PUT /_index_template/my_template
  {
    "index_patterns": ["my_index*"],
    "composed_of": ["my_pipeline"],
    "priority": 1
  }
  ```

## 使用建议

在使用Elasticsearch Pipeline时，有几点建议可以帮助提高效率和准确性：

- **测试和验证**：在应用Pipeline之前，务必进行充分的测试和验证，确保处理步骤的准确性和稳定性。
- **监控和调优**：定期监控Pipeline的性能和效果，根据实际情况进行调优和优化，以提高数据处理和索引效率。
- **复用Pipeline**：针对相似的数据处理需求，可以设计通用的Pipeline，以便在多个索引中重复使用，提高代码复用性和维护性。
- **合理使用条件**：根据具体需求选择合适的条件触发Pipeline的应用，避免不必要的处理过程，提高系统性能。
